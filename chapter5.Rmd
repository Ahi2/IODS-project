---
title: "chapter5.Rmd"
author: "Ahi"
date: "2 joulukuuta 2017"
output: html_document
---

# Dimensionality reduction techniques

In this exercise we use a real dataset developed by the UN, which is developed to measure the human development of countries, in order to perform Principle Component Analysis (PCA), a method to reduce the number of measured (continuous) and correlated variables into a few uncorrelated components that collect together as much variance as possible from the original variables. 

```{r}
library(GGally)
library(dplyr)
library(corrplot)
library(ggplot2)
library(tidyr)
```

# 1. Loading data into R

 
First we explore the structure and the dimensions of the dataset, calling it 'human2'.

```{r}
human2 <- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human2.txt", sep  =",", header = T)

str(human2)
dim(human2)
summary(human2)
```

# 2. Graphical overview of the data and summary of the variables

Now we look at the normal disctributions, correlationshs, and a graphical overview of the dataset.  
```{r}
ggpairs(human2)
```

As an example to explore the graph above, We can see that expected years of schooling (Edu.Exp) has almost a normal distrbution. We can also understand that there is quite a strong positive relationship between female/male ratio of secondary education (Edu2.Fm) and life expectancy at birth (Life.Exp).  

Let us also look at the correlations of the variables.
```{r}
cor(human2)
```

```{r}
cor(human2) %>% corrplot
```

As can be seen from the correlations, for instance, adolescent birth rate (Ado.Birth) and Maternal mortality ratio (Mar.Mor) have a strong positive corrleation, which is unsurprising. Less unsurpersing is the negative strong relationship between life expectancy at birth (Life.Exp) and Mat.Mor.

# 3. Principle Component Analysis on the not standardized variables
```{r}
pca_human2 <- prcomp(human2)

biplot(pca_human2, choices = 1:2, cex = c(0.8, 1), col = c("grey40", "deeppink2"))

```


As can be seen from the graph above, when the variables are not standardized, the results are not promising. Now we have one dimension explaning the variance of the whole dataset. This is because PCA is an unsupervised method of dimension reduction technique. Second, PCA is sensitive to the relative scaling of the original variable, assuming that features with larger variace are more important than the ones with smaller variance. To address this issue, one way is to standardize the data. We do this next.  

# 4. Principle Component Analysis on the standardized variables
By standardizing the data, all the variables will have the mean of 0 and standard deviation of 1. This will lead to a more realistic PCA. 
  
```{r}
human2_std <- scale(human2)

summary (human2_std)

```

Now that the varables are standardized, let's perform PCA with the Singular Value Decomposition (SVD) method. 
```{r}
pca_human2 <- prcomp(human2_std)

biplot(pca_human2, choices = 1:2, cex = c(0.8, 1), col = c("grey40", "deeppink2"))
```

As can be seen from the plot above, now we have more realistic and precise explanations of the variance of the variables by the two components. But, we can still improve the plot further to show how much of the variance is explained by the two components. 

```{r}
s <- summary (pca_human2)

s
```

```{r}
pca_pr <- round(100*s$importance[2, ], digits = 1)
pca_pr
```

```{r}
pc_lab <- paste0(names(pca_pr), " (", pca_pr, "%)")
```

Now, we draw a biplot of the principal component representation and the original variables.
```{r}
biplot(pca_human2, cex = c(0.8, 1), col = c("grey40", "deeppink2"), xlab = pc_lab[1], ylab = pc_lab[2])
```

Now all the variables of the study have been reduced to two principle components (PC1 and PC2). PC1 explains 53.6% of the variance, while PC2 explains 16.2% of the variance. The Biplot implies that, for instance, Labor.FM (labor, female/male ratio ) is almost orthogonal (uncorrelated) with Edu.Exp (expected years of schooling). We can see this in the correlation matrix and graph, run in section 2. On the other hand, we can infer from the graph that Labor.FM and Parli.F (percentage of female representatives in parliament) are correlated, which is not surprising. 

Further, we can understand from the plot that Labor.FM and Parli.F are contributing to the second dimension, PC2. The same holds true for other variables, which contibute to PC1. The reason is that, those features contribute to the dimensions with similar direction. As PC1 includes more variables, it explains more of the variance of the data.

# 5. Interpretation of the first two principle components
As I explained in the above, PC2 explains less of the variance. It includes the variables, Labor.FM and Parli.F. Both of these two variables relate to the involvment of females in a given society. The higher the values, the higher the involvment of females. These two variables unsurprisingly also correlate with one another. Overall, we could refer to PC2 as gender development in the society. This is the second big component (also referred to as "factor"") of our dataset.

The first component, which explains the majority of the variance, includes variables such as Expected years of schooling, Life expectancy at birth, and Adolescent birth rate. They mainly refer to health and education, representing a multidimensional index, which indicates the population health, reproduction health index, and access to educational resources. 

# 6. The tea time!: looking at the tea dataset
The tea dataset already exists in a package called FactoMiner in R. First we load the package, and then we'll look at the structure and dimensions of the "tea"" dataset. 

```{r}
library(FactoMineR)
```

```{r}
data(tea)
str(tea)
dim(tea)
```

As can be seen the dataset includes 300 observations and 36 variables.

Now we select some variables on which to perform a PCA. I select the following variables, creating a new dataset, calling it tea_time: 
```{r}
keep_columns <- c("Tea", "How", "how", "sugar", "where", "lunch")

tea_time <- select(tea, one_of(keep_columns))
```

Now let's look at the summary and strucutre of the new dataset:
```{r}
summary(tea_time)
str(tea_time)
```

Now let's visualize the dataset.
```{r}
gather(tea_time) %>% ggplot(aes(value)) + geom_bar() + facet_wrap("key", scales = "free") + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8)) 
```

As an example, if we look at how the tea is consumed we can see that through tea bag. As another example, the last graph shows that tea is consumed mainly in chain stores. 

Now let's look at the Multiple Correspondence Analysis of the dataset, a method to analyze qualitative data, an extension of Correspondence analysis (CA). 

```{r}
mca <- MCA(tea_time, graph = FALSE)
summary(mca)

```

Now, let's visualise our Multiple Correspondence Analysis 
```{r}
plot(mca, invisible=c("ind"), habillage = "quali")
```

We can see from the plot above that two dimensions (1 and 2) explain 15.24% and 14.23% of the variance, respectively. Both Dimesions, to some extent, explain similar variables. For example, dimension 1 includes variables such as the type of tea (i.e., back, green or earl gray), how the tea is consumed (teabag or unpackaged), and where tea is consumed (e.g., chainstore or tea shop). Dimension 2 also explains this. The distinguishing factor of dimension 2 compared to 1 is that dimension 2 explains How tea is consumed (in terms of what is added to tea: sugar, raw or, e.g., lemon) better than dimension 1. In general, it seems that this dataset could have more dimensions, as other dimensions such as 3 and 4 also explan considerable portion of the variance.   

