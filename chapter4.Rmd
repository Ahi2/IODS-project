---
title: "chapter4.Rmd"
author: "Ahi"
date: "25 marraskuuta 2017"
output: html_document
---

# Clustering and classification

In this exercise, we use Boston dara, already loaded in r. But, first let's access some libraries we will need during the exercise. 

```{r}
library(tidyr)
library(ggplot2)
library(corrplot)
library(GGally)
```

After creating the RMarketdown file, we load and explore the Boston dataset. 

# 2. Loading the Boston data and exploring the data

```{r}
library(MASS)
data("Boston")
str(Boston)
summary(Boston)
```

According to the structure of the data, Boston dataset, which is already loaded in R, has 506 observattions (rows) and 14 variables (columns). The data are mainly gathered to understand the effect of housing values in suburbs of Boston on different vairbles such as crime rate.  

# 3. Graphical overview of the data and summary of variables 

```{r}
pairs(Boston)
```

The plot above shows the correlations of all the variables in the dataset. But let's look at a more advanced plot to see the distribution of the data as well as correlation of the variables.

```{r}
p <- ggpairs(Boston, mapping = aes(alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))
p
```

The above plot demonstrates the distribution of the data as well as the correlations of the variables. As an example, data in the variable "rm", average number of room per dwelling, have normal distribution, and there is a positive, and rather strong, correlation between "zn" (proportion of residential land zoned for lots over 25,000 sq.ft.), and "dis" (weighted mean of distances to five Boston employment centres).

But we can look at the corraltions with more r funcations. Let's explore cor_matrix function, which provides handy, easy-to-interpret-correlation matrix. 

```{r}
cor_matrix<-cor(Boston) %>% round(digit=2)

cor_matrix %>% round(cor_matrix)

corrplot(cor_matrix, method="circle", type = "upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)
```

Let's explore some examples. We can see that there is a strong and positive relationship between "rad" (index of accessibility to radial highways) and "tax" (full-value property-tax rate per \$10,000). In addition, there is a negtive and strong relationship between "lstat" (lower status of the population (percent)) and "medv" (median value of owner-occupied homes in \$1000s). 

# 4. Standardizing the dataset

```{r}
boston_scaled <- scale(Boston)
summary(boston_scaled)
class(boston_scaled)
boston_scaled <- as.data.frame(boston_scaled)
```

Creating a categorical variable of the crime rate in the Boston dataset. First, let's create a quantile vector of crime rate.

```{r}
bins <- quantile(boston_scaled$crim)
bins
```

Now, let's create a categorical variable, and call it, 'crime':
```{r}
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, c(label = "low", "med_low", "med_high", "high"))
table(crime)
```

Now, let's remove the original "crim"" from the dataset
```{r}
boston_scaled <- dplyr::select(boston_scaled, -crim)
```

And add the new categorical value to scaled data
```{r}
boston_scaled <- data.frame(boston_scaled, crime)
```

Dividing the dataset to train and test sets, so that 80% of the data belongs to the train set. 
```{r}
n <- nrow(boston_scaled)
ind <- sample(n,  size = n * 0.8)
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,]
correct_classes <- test$crime
test <- dplyr::select(test, -crime)
```

# 5. Fitting the linear discriminant analysis (LDA) on the train dataset 

Use the crime as a target variable and all the other variables as predictors.
```{r}
lda.fit <- lda(crime ~ ., data = train)
lda.fit
```

Drawing the LDA (bi)plot. But first let's create a numeric vector of the train sets crime classes.
```{r}
classes <- as.numeric(train$crime)
```

Now we're ready to draw the LDA plot.
```{r}
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 3)
```

# 6. Predicting the classes with the LDA model

In part 4, categorical crime variable has already been removed from the test dataset, and we have now the test data and correct class labels. Now, we predict the classes with the LDA model on the test data.
```{r}
lda.pred <- predict(lda.fit, newdata = test)
```

Cross tabulating the results by creating a table of the correct classes and the predicted ones.
```{r}
table(correct = correct_classes, predicted = lda.pred$class)
```
The table above reveals that in most cases the crime rates have been predicted correctly, but there are some inconsistencies between the predicted results and correct ones. It seems that the class, high (crime rates), is the most precisely predicted one.

# 7. Calculating the distances between the observations

First, reloading and standardizing the dataset.
```{r}
data('Boston')
boston_scaled <- scale(Boston)
boston_scaled <- as.data.frame(boston_scaled)
```

Now, let's use dist() function to calculate the distances between observation using the most common distance measure, which is Euclidean distance.
```{r}
dist_eu <- dist(Boston)
summary (dist_eu)
```
Now, let's use manhattan distance matrix, another distance measure.
```{r}
dist_man <- dist(Boston, method = "manhattan")
summary(dist_man)
```

Now, we investigate the optimal number of clusters, using K-means clustering. First, we run K-means algorithm on the data. 
```{r}
km <-kmeans(Boston, centers = 4)
pairs(Boston, col = km$cluster)
```
Now, we investigate the optimal number of clusters using K-means clustering, with 10 clusters. One way to specify the number of clusters is to look at how the total of within cluster sum of squares (WCSS) behaves when the number of cluster changes. So, let's look at the behavior of WCSS and plot it. 

```{r}
set.seed(123)
k_max <- 10
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})
qplot(x = 1:k_max, y = twcss, geom = 'line')
```

By visualizing the total WCSS as a graph, we can see that two clusters would seem optimal. Becasue, the optimal number of clusters is when the value of total WCSS changes radically.  
So, let's run k-means again with two clusters and then visualize the results.

```{r}
km <-kmeans(Boston, centers = 2)
pairs(Boston, col = km$cluster)
```

# Creating a 3D plot 
```{r}
model_predictors <- dplyr::select(train, -crime)
```
check the dimensions

```{r}
dim(model_predictors)
dim(lda.fit$scaling)
```

matrix multiplication
```{r}
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)
```

```{r}
library(plotly)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers')
```
