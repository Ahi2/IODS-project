---
title: "chapter3.Rmd"
author: "Ahi"
date: "19 marraskuuta 2017"
output: html_document
---

library(tidyr)

library(dplyr)

library(ggplot2)

# 2. Reading the joined dataset

```{r}
alc <- read.table("C:/Data/IODS-project/alc.csv", sep=",", header=TRUE)
```

This is a dataset which contains information on students achievment in two classes, math and Portuguese language. The aim is to see, e.g., how alocohol consumtpion and gender affect students' performance. The name of the variables used can be seen here:

```{r}
colnames(alc)
```

# 3. The hypotheses regarding four variables and alc consumption

Now we want to study the effect of students' gender, age, health, and final grade (G3) on students' alcohol consumption...we use high_consumption as the dependent variable...

# 4. exploring the distribution of the variables 
Let's look at the disribution of the variables selected:

```{r}
g1 <- ggplot(alc, aes(x = high_use)) + facet_wrap("sex")

g1 + geom_bar()

```

```{r}
g2 <- ggplot(alc, aes(x = high_use, y = G3, col = sex))

g2 + geom_boxplot() + ylab("grade")

g3 <- ggplot(alc, aes(x = high_use, y = health, col = sex)) + ggtitle("Student health by alcohol consumption and sex")

g3 + geom_boxplot() + ylab("health")

g4 <- ggplot(alc, aes(x = high_use, y = age)) + ggtitle("Student age by alcohol consumption")

g4 + geom_boxplot() + ylab("age")
```

# 5. Logistic regression

looking at the logistic model, where alcohol consumption is the depedent variables, which is binory (0 and 1), and gender, health, age, and grade are independent variables. 
```{r}
m <- glm(high_use ~ sex + G3 + health + age, data = alc, family = "binomial")
```

printing out the model:
```{r}
m
```

coefficients and the summary of the model

```{r}
coef(m)
summary(m)
```
Computing odds ratios (OR)

```{r}
OR <- coef(m) %>% exp
```

computing confidence intervals (CI)

```{r}
CI <- confint(m) %>% exp
```
Printing out the odds ratios with their confidence intervals
```{r}
cbind(OR, CI)
```
As can be seen from the results, the gender has the widest confidence interval. 

# 6. Exploring the predictive power of the model

Using the variables which were statistically signifacnt in a model called m2 
```{r}
m2 <- glm(high_use ~ sex + G3 + age, data = alc, family = "binomial")
```

looking at the predictive power of m2

```{r}
probabilities <- predict(m, type = "response")
```

adding the  predicted probabilities to 'alc'
```{r}
alc <- mutate(alc, probability = probabilities)
```

using the probabilities to make a prediction of high_use
```{r}
alc <- mutate(alc, prediction = probability > 0.5)
```

Looking at the last ten original classes, predicted probabilities, and class predictions
```{r}
select(alc, sex, G3, age, high_use, probability, prediction) %>% tail(10)
```

tabulating the target variable versus the predictions
```{r}
table(high_use = alc$high_use, prediction = alc$prediction)
```

displaying the actual values and the predictions
```{r}
g <- ggplot(alc, aes(x = probability, y = high_use, col = prediction))

g + geom_point()
```

Computing the total proportion of inaccurately classified individuals 
```{r}
loss_func <- function(class, prob) {
  n_wrong <- abs(class - prob) > 0.5
  mean(n_wrong)
}

loss_func(class = alc$high_use, prob = alc$probability)
```
 
# 7 and 8. Performing cross-validation to compare different models

library(boot)
```{r}
cv <- cv.glm(data = alc, cost = loss_func, glmfit = m, K = 10)
cv$delta[1]
```
After performing this 10-fold crosss-validation test, we can see that the prediction error is larger than the model in the DataCamp (~ 0.29 > 0.26). Therefore, it seems that this model does not perform better. Let's now use m2 model, in which one of the variables (health) that was insignificant is removed.

```{r}
cv <- cv.glm(data = alc, cost = loss_func, glmfit = m2, K = 10)
cv$delta[1]
```
Though the prediction error of this model is slightly better than the other one (m1), it still is larger than the one in DataCamp. So, let's remove the variable "age", and keep only the "sex" and "G3", which are more significant statistically. Let's call the model, m3:

```{r}
m3 <- glm(high_use ~ sex + G3, data = alc, family = "binomial")
```

Now, let's look at the prediction error of m3:

```{r}
cv <- cv.glm(data = alc, cost = loss_func, glmfit = m3, K = 10)
cv$delta[1]
```
Still the prediction error is larger. So, let's introduce new variables to the model: absences and sex, and remove G3.   

```{r}
m4 <- glm(high_use ~ sex + absences, data = alc, family = "binomial")

cv <- cv.glm(data = alc, cost = loss_func, glmfit = m4, K = 10)
cv$delta[1]
```

As can be seen, now the predictive error of the model is smaller than the previous models. It is however the same as the one in the DataCamp exercise.

So, let's modify our hypotheses and develop new logistic model - with different variables - to test whether our model improves. Accordingly, in the follwing, I introduce explanatory variables to the model, while keeping the dependent variables (high_use) the same. I start with large number of variables and continue with smaller number.

m5 - explanatory variables: sex, age, studytime, failures, famsup, goout (going out with friends), famrel (quality of family relationship), health, and absences.

```{r}
m5 <- glm(high_use ~ sex + age + studytime + failures + famsup + goout + famrel + health + absences, data = alc, family = "binomial") 
```

Now, let's look at a summary of the model as well as the coefficients.
```{r}
summary(m5)

coef(m5)
```

Let us now look at the predictive power of the model, by estimating the prediction error:
```{r}
cv <- cv.glm(data = alc, cost = loss_func, glmfit = m5, K = 10)
cv$delta[1]
```

As the model summary reveals, the variables, sex, studytime, goout, famrel, and absences have statistically significant relationship with alocohol consumption (high_use). Also, the prediction error of the model now is smaller than previous models. It is also smaller than the model in the DataCamp. Therefore, this is a better model in general.

Now let us look at the boxplot of some of the variables in the model:

```{r}
g5 <- ggplot(alc, aes(x = high_use, y = studytime, col = sex)) + ggtitle("Student study time and by alcohol consumption and sex")

g5 + geom_boxplot() + ylab("study time")

g6 <- ggplot(alc, aes(x = high_use, y = famrel, col = sex))

g6 + geom_boxplot() + ylab("family relationship")
```

Now let's make the model even better by keeping only significant predictors (reducing variables), and call it m6. 

```{r}
m6 <- glm(high_use ~ sex + studytime + goout + famrel + absences, data = alc, family = "binomial")

summary(m6)

coef(m6)

cv <- cv.glm(data = alc, cost = loss_func, glmfit = m6, K = 10)
cv$delta[1]
```

As the result shows now the predictive error of the model is smaller, hence a better model. All the variables in this model are now significant. The relationships of these variables can be explaiend as follows:

Sex (gender) is positively related to alcohol consumprion; stated differently, male students are more likely to consume high amount of alcohol, compared to to female students. 

study time is negatively correlated with high alcohol consumption. This makes sense, as the more students drink alcohol the less they spend time studying. 

Going out with friends (gout) is positively and highly correlated with high alcohol consumption. This is not surprising as those students who go our with their friends ore often are more likely to drink high amount of alcohol. 

Interesingly, and as expected, quality of family relationship (famrel) is negatively correlated with high alcohol consumption. This means that students living with a supportive family, wherein they enjoy quality relationship, are less likely to drink high amount of alocohol. For example, when facing with stressful circumstances, instead of drinking, they may socialize with their family members, which helps reduce stress.

Finally, as also anticipated, the number of school absences are positively correlated with alcohol consumption. For instance, hangover in the morning may be one reason for high number of absences. 


