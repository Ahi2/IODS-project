---
title: "chapter3.Rmd"
author: "Ahi"
date: "19 marraskuuta 2017"
output: html_document
---

# Logistic Regression 

In this exercise, I am going to perform logistic regressoin to see the effect of different variabes on students' alcohol consumption. I have combined Math-Por dataset and called alc, which includes data from students in both mathematics and Portugies classes. First, let us run some of the libraries we need. 

```{r}
library(tidyr)

library(dplyr)

library(ggplot2)

library(magrittr)
```

# 2. Reading the joined dataset

```{r}
alc <- read.table("C:/Data/IODS-project/alc.csv", sep=",", header=TRUE)
```

This is a dataset which contains information on students achievment in two classes, math and Portuguese language. The aim is to see, e.g., how alocohol consumtpion and gender affect students' performance. The name of the variables in the dataset can be seen here:

```{r}
colnames(alc)
```

# 3. The hypotheses regarding four variables and alc consumption

In this exercise, I am interested in studying the effect of students' gender, age, health, and final grade (G3) on students alcohol consumption. Since the dependent variable is categorical (high vs. low consumption), logistic regression analysis is used. The following are my hypotheses:

H1. Male studnets are more likely to consume high amount of alcohol compared with their female classmates

H2. There is a positive relationship between age and alcohol consumption. As such, the older the
students the higher the likelihood of high alcohol consumption.

H3. There is a negative relationship between students' health and high alcohol consumption.

H4. There is a negative relationship between students' grades and high alcohol consumption. 

# 4. exploring the distribution of the variables 

Let's look at the disribution of the variables selected:

```{r}
alc %>% group_by(sex, high_use) %>% summarise(count = n(), mean_grade = mean(G3))
```

Now we can see that the number of female students who have not drunk high amount of alcohol (False) is larger than those who have (True), and the grade of the former group is slightly higher than that of the latter. The same holds true for male students; the difference is that the number of male students who reprted high alcohol consumption is larger than that of female students, and the difference between thei grades are more significant (~ 10.27 for those who drink, and ~ 12.2 for those who do not).  

Now let's look at some boxplot of the variables
```{r}
g1 <- ggplot(alc, aes(x = high_use, y = G3, col = sex))
  

g1 + geom_boxplot() + ylab("grade")

```

```{r}
g2 <- ggplot(alc, aes(x = high_use, y = health, col = sex)) + ggtitle("Student health by alcohol consumption and sex")

g2 + geom_boxplot() + ylab("health")
```

```{r}
g3 <- ggplot(alc, aes(x = high_use, y = age)) + ggtitle("Student age by alcohol consumption")

g3 + geom_boxplot() + ylab("age")
```

# 5. Logistic regression

In this section,let us look at the logistic model, wherein alcohol consumption is the depedent variables, which is binery (0 and 1), while gender, health, age, and grade are independent variables. 
```{r}
m <- glm(high_use ~ sex + G3 + health + age, data = alc, family = "binomial")
```

printing out the model:
```{r}
m
```

coefficients and the summary of the model

```{r}
coef(m)
summary(m)
```

Now, let us Compute odds ratios (OR)

```{r}
OR <- coef(m) %>% exp
```

computing confidence intervals (CI)

```{r}
CI <- confint(m) %>% exp
```
Printing out the odds ratios with their confidence intervals
```{r}
cbind(OR, CI)
```
As can be seen from the results, the gender has the widest confidence interval. 

Hypotheses test results:

The results reveal that gender is a significant variable. According to this and the distribution of this variable, shown earlier, male studnets are more likely to drink high amount of alcohol. This confirms my 1st hypothesis.

There is a positive and significant (though weak) relationship between students' age and high alcohol consumption, confirming the 2nd hypothesis.

There is no relationship between students' health and high alcohol consumption, rejecting my 3rd hypothesis. This could be due to the fact that students are very young, hence alcoholc consumption does not affect their health yet.

There is a negative, significant relationship between students' grade and high alcohol consumption, confirming the 4th hypothesis.  

# 6. Exploring the predictive power of the model

Using the variables which were statistically signifacnt in the previous model, hence removing "health". Let's call this m2.
```{r}
m2 <- glm(high_use ~ sex + G3 + age, data = alc, family = "binomial")
```

looking at the predictive power of m2

```{r}
probabilities <- predict(m2, type = "response")
```

adding the  predicted probabilities to 'alc'
```{r}
alc <- mutate(alc, probability = probabilities)
```

using the probabilities to make a prediction of high_use
```{r}
alc <- mutate(alc, prediction = probability > 0.5)
```

Looking at the last ten original classes, predicted probabilities, and class predictions
```{r}
select(alc, sex, G3, age, high_use, probability, prediction) %>% tail(10)
```

tabulating the target variable versus the predictions
```{r}
table(high_use = alc$high_use, prediction = alc$prediction)
```

displaying the actual values and the predictions
```{r}
g <- ggplot(alc, aes(x = probability, y = high_use, col = prediction))

g + geom_point()
```

Computing the total proportion of inaccurately classified individuals 
```{r}
loss_func <- function(class, prob) {
  n_wrong <- abs(class - prob) > 0.5
  mean(n_wrong)
}

loss_func(class = alc$high_use, prob = alc$probability)
```
 
# 7 and 8. Performing cross-validation to compare different models


```{r}
library(boot)
cv <- cv.glm(data = alc, cost = loss_func, glmfit = m, K = 10)
cv$delta[1]
```
After performing this 10-fold crosss-validation test, we can see that the prediction error is larger than the model in the DataCamp (~ 0.28 > 0.26). Therefore, it seems that this model does not perform better.
So, let's remove the variable "age", and keep only the "sex" and "G3", which are more significant statistically. Let's call the model, m3:

```{r}
m3 <- glm(high_use ~ sex + G3, data = alc, family = "binomial")
```

Now, let's look at the prediction error of m3:

```{r}
cv <- cv.glm(data = alc, cost = loss_func, glmfit = m3, K = 10)
cv$delta[1]
```
Still the prediction error is large. So, let's introduce new variables to the model: absences and sex, and remove G3.   

```{r}
m4 <- glm(high_use ~ sex + absences, data = alc, family = "binomial")

cv <- cv.glm(data = alc, cost = loss_func, glmfit = m4, K = 10)
cv$delta[1]
```

As can be seen, now the predictive error of the model is smaller than the previous models. It is however the same as the one in the DataCamp exercise.

So, let's modify our hypotheses and develop a new logistic regression model - with different variables - to test whether or ot our model improves. Accordingly, in the follwing, I introduce explanatory variables to the model, while keeping the dependent variables (high_use) the same. I start with a large number of variables and continue with smaller number.

m5 - explanatory variables: sex, age, studytime, failures, famsup, goout (going out with friends), famrel (quality of family relationship), health, and absences.

```{r}
m5 <- glm(high_use ~ sex + age + studytime + failures + famsup + goout + famrel + health + absences, data = alc, family = "binomial") 
```

Now, let's look at a summary of the model as well as the coefficients.
```{r}
summary(m5)

coef(m5)
```

Let us now look at the predictive power of the model, by estimating the prediction error:
```{r}
cv <- cv.glm(data = alc, cost = loss_func, glmfit = m5, K = 10)
cv$delta[1]
```

As the model summary reveals, the variables, sex, studytime, goout, famrel, and absences have statistically significant relationship with alcohol consumption (high_use). Also, the prediction error of the model now is smaller than previous models. It is also smaller than the model in the DataCamp. Therefore, this is a better model in general.

Now let us look at the boxplot of some of the variables in the model:

```{r}
g5 <- ggplot(alc, aes(x = high_use, y = studytime, col = sex)) + ggtitle("Student study time and by alcohol consumption and sex")

g5 + geom_boxplot() + ylab("study time")

g6 <- ggplot(alc, aes(x = high_use, y = famrel, col = sex))

g6 + geom_boxplot() + ylab("family relationship")
```

Now let's make the model even better by keeping only significant predictors (reducing variables), and call it m6. 

```{r}
m6 <- glm(high_use ~ sex + studytime + goout + famrel + absences, data = alc, family = "binomial")

summary(m6)

coef(m6)

cv <- cv.glm(data = alc, cost = loss_func, glmfit = m6, K = 10)
cv$delta[1]
```

As the results show now the predictive error of the model is smaller (~ 0.22), hence a better model. All the variables in this model are now significant. The relationships of these variables can be explaiend as follows:

Sex (gender) is positively related to alcohol consumprion; stated differently, male students are more likely to consume high amount of alcohol, compared with female students. 

Study time is negatively correlated with high alcohol consumption. This is logical. The more students drink alcohol the less they spend their time studying. 

Going out with friends (gout) is positively and highly correlated with high alcohol consumption. This is not surprising, as those students who go out with their friends more often, are more likely to drink high amount of alcohol. 

Interesingly, and as expected, quality of family relationship (famrel) is negatively correlated with high alcohol consumption. This means that students living with a supportive family, wherein they enjoy good social relationship, are less likely to drink high amount of alocohol. For example, when facing with stressful circumstances, instead of drinking, they may socialize with their family members, which helps reduce stress.

Finally, as also anticipated, the number of school absences are positively correlated with alcohol consumption. For instance, hangover in the morning may be one reason for high number of absences. 

